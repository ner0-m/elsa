This is a collection of thoughts on how the ml module could be improved in the
future. 

Unified backends
----------------
Currently the Cudnn and Dnnl backends are completely distinct, i.e., they don't
share a common base class. This could easily be changed in the future by
introducing a common memory type such as our current CudnnMemory. This could be
a simple type holding pointers to host and device memory.

Unified graph types
-------------------
Ideally we don't operate on layers that are stored in a graph but on abstract
graph nodes. These node can represent layers but also whole models (since models
can also be inputs for other layers).

Furthermore the execution of an operation should be separated from a layer
implementing it just like e.g. the STL separates containers from algorithms.
Layers would be stateful objects holding data. Using a unified memory type no 
distinction between backend would be necessary. Operations would be stateless
and executed on layers and would be implemented per backend. This would also
simplify the forward and backward logic of the graph.

    // Same for both backends
    class DenseLayer {
      Memory* input;
      Memory* weights;
    };

    Memory* dense_cpu(DenseLayer* layer) {
      // do Dnnl stuff
    }

    Memory* dense_gpu(DenseLayer* layer) {
      // do Cudnn stuff
    }


Implement callback logic
------------------------
A simple callback logic could be implemented that hooks into defined points of
the training.

Have a separate inference path
------------------------------
Currently we (ab)use training forward propagation for inference even though
inference can need substantially less resources as forward propagation (there is
e.g. no need to store intermediate states for pooling). The frameworks partially
support separate inference primitives that could be used. This would be
especially helpful for scenarios that just load pre-trained models with the goal
of prediction only (a classical example is transfer learning).

Implement support for narrow types
-----------------------------------------
Support for float16 (also called half-floats) and bfloat16 could be implemented.
Especially the latter could be beneficial for reconstruction tasks but supporting
hardware is still rare. It is not feasible at all to fully implement models in
narrow types but one usually uses different types for different operations. This
would thus also require handling of mixed-type networks. The standard approach
to this is erasing the C++ datatype (e.g. by using std::shared_ptr<void> or just
void*) and encoding the type into a type-tag. Whenever a real type is needed
the tag is translated back into the type.
